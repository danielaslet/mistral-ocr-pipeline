
This readme file explains how to use the PDF to content understanding workflow:

â¸»

ğŸ“˜ Azure Content Understanding Batch Processor

This project runs Azure Content Understanding against a batch of PDF files stored in an Azure Blob Storage container. It generates three output files per input:
	â€¢	A Markdown version of the extracted content
	â€¢	A Styled HTML version for human-friendly viewing
	â€¢	A JSON log of the raw service response

â¸»

ğŸ“ File Overview

1. analyze_content_understanding_blob_batch.py

This is the main script. It:
	â€¢	Lists all .pdf files from an Azure Blob Storage container (optionally scoped to a folder prefix)
	â€¢	Constructs URLs with a shared SAS token
	â€¢	Sends each file to Azure Content Understanding
	â€¢	Waits for the job to complete and downloads the result
	â€¢	Extracts the raw Markdown and converts it to styled HTML
	â€¢	Saves output files in structured folders

2. analyze_content_understanding_helpers.py

This contains shared helper functions to:
	â€¢	Extract markdown from the raw API response
	â€¢	Convert markdown to HTML with optional regex visualization rules
	â€¢	Save markdown, HTML, and JSON log files

You should not need to modify this file.

3. analyze_content_understanding_blob_config.yaml

The configuration file used by the script. It lets you define:
	â€¢	The blob container URL and optional folder prefix
	â€¢	A shared SAS token (read+list permissions required)
	â€¢	Analyzer ID to use (e.g., "prebuilt-documentAnalyzer")
	â€¢	Output folders for Markdown, HTML, and JSON logs
	â€¢	Optional regex-based HTML visualization rules

â¸»

âš™ï¸ How to Use

âœ… Step 1: Prepare Your Files
	â€¢	Upload your PDFs to an Azure Blob Storage container
	â€¢	Make sure theyâ€™re readable with the SAS token

âœ… Step 2: Configure the YAML

Update analyze_content_understanding_blob_config.yaml like so:

blob_container_url: "https://<account>.blob.core.windows.net/<container>"
prefix: ""  # or "some/folder" if using a virtual directory
sas_token: "sp=r&st=...&se=...&sv=...&sr=c&sig=..."

analyzer_id: "prebuilt-documentAnalyzer"

log_output_dir: ./outputs/logs
markdown_output_dir: ./outputs/markdown
html_output_dir: ./outputs/html

enable_visualization: true
visualization_rules:
  - condition: '(?m)<!--\\s*(.*?)\\s*-->'
    treatment: '<p style="color:blue;">--- COMMENT: \\1 ---</p>'

âœ… Note: sas_token must have r (read) and l (list) permissions to work properly.

âœ… Step 3: Run the Script

python analyze_content_understanding_blob_batch.py

The script will:
	â€¢	List all .pdf files in the specified container/prefix
	â€¢	Print how many files were found and their names
	â€¢	Show progress using â€œ(x of N)â€ while processing each file
	â€¢	Save outputs with filenames matching the original PDF names:
	â€¢	document.pdf â†’ document.md, document.html, document.json

â¸»

ğŸ“‚ Example Output Structure

outputs/
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ some_file.json
â”œâ”€â”€ markdown/
â”‚   â””â”€â”€ some_file.md
â””â”€â”€ html/
    â””â”€â”€ some_file.html


â¸»

ğŸ’¡ Notes
	â€¢	HTML output includes styled tables and optional visual indicators based on regex rules
	â€¢	File names in all outputs exactly match the input PDF name (only the extension changes)
	â€¢	If you want to skip already-processed files or track progress in a manifest, this can be added

â¸»

Let me know if youâ€™d like to include automated upload of outputs back to Blob Storage, error handling summaries, or a CSV manifest of results.